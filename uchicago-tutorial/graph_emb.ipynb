{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing Exact Graph Embeddings with PyTorch\n",
    "\n",
    "In this tutorial we will learn how to solve the graph reconstruction problem using PyTorch. We will reconstruct our graph's full adjacency matrix, and we will frame this problem as a logistic regression.\n",
    "\n",
    "Namely, we want to ask, given two node IDs, is there going to be an edge between them or not? This would be a classical logistic regression problem, **if only we had some features!**\n",
    "\n",
    "The good news is that we can **learn our own features** to solve this problem. To a social scientist this notion may seem a bit counter-intuitive. How can we learn about variables (\"features\") we did not even have an idea existed?\n",
    "\n",
    "One way to understand the feature learning approach is to think about the missing data problem. There are many techniques to do imputation of missing values in survey datasets for instance. In a sense, this is what we'll be doing, only that we will be imputing values over entire columns of data.\n",
    "\n",
    "This may seem a bit like black magic. Normally, when we impute missing data, we look at existing values in our data matrix for guidance on what values we should assign to empty cells. In our case the only information we have is whether there exists a tie between two edges or not. As it turns out, this is enough. \n",
    "\n",
    "## Problem setup\n",
    "\n",
    "For every node $i$ we assume there exists a vector $\\vec{v}_i$ of dimensionality $d$. This vector is the *feature vector* and will encode all the information we learn about the node's participation in the graph. We will randomly sample pairs of nodes $(i, j)$ from the graph's adjacency matrix.\n",
    "\n",
    "Let $Y_{i, j}$ an indicator variable denoting the existence of an edge between nodes $i$ and $j$. We will then write:\n",
    "\n",
    "$$ Y_{i, j} = \\mathrm{sigmoid}(\\vec{v}_i \\cdot \\vec{v}_j) + \\epsilon_{i, j}, $$\n",
    "\n",
    "where $\\mathrm{sigmoid}$ is the sigmoid function, $\\frac{1}{1 + e^{-x}}$.\n",
    "\n",
    "We will seek to reduce the values of $\\epsilon_{i, j}$ as much as possible, under certain constraints expressed by a  **loss function**. In our case (logistic regression), this simply the [log loss](http://wiki.fast.ai/index.php/Log_Loss) function, also known among neural network researchers as the **binary cross entropy loss**:\n",
    "\n",
    "$$ L = - \\sum_{i} \\sum_{j} y_{i, j} \\log{p_{i, j}} + (1-y_{i, j})\\log({1-p_{i,j}}) $$\n",
    "\n",
    "Here, $p_{i, j} = \\mathrm{sigmoid}(\\vec{v}_i \\cdot \\vec{v}_j)$.\n",
    "\n",
    "The quantity $L$ presented above is an unweighted loss. Such a loss quantifies how well we're doing at predicting our edges. Most real-world graphs are sparse, however, meaning they have many fewer 1s than 0s in their adjacency matrices. In this case, we will want to compute a *weighted* loss:\n",
    "\n",
    "$$ L = - \\sum_{i} \\sum_{j} w_{i, j} \\big[y_{i, j} \\log{p_{i, j}} + (1-y_{i, j})(\\log{1-p_{i,j}})\\big] $$\n",
    "\n",
    "The goal of introducing $w_{i, j}$ is to ensure that doing poorly at predicting 1s in our matrix is penalized just as heavily as doing poorly at predicting 0s. Thus if $N_0$ and $N_1$ quantify the number of 1s and 0s in our adjacency matrix, then we can set:\n",
    "\n",
    "$$\n",
    "w_{i, j} = \\begin{cases}\n",
    "1 & Y_{i, j} = 1 \\\\\n",
    "\\frac{N_1}{N_0} & Y_{i, j} = 0 \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Now that we have the apparatus behind our loss function, let's take a look at how we will be computing the values of this function in practice. For now we won't worry about how we will actually minimize this loss, nor will we pay much attention to computational efficiency.\n",
    "\n",
    "We will start by importing `torch` and `numpy`, the two packages we will use heavily in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will load an example graph to play with. We will only look at a small graph, an email network of about 1000 persons from a EU institution, which we can obtain from [SNAP](https://snap.stanford.edu/data/email-Eu-core.html). Make sure to download the `email-Eu-core` file and unzip it (via `gunzip` in UNIX-like systems). Then change the `DATA_PATH` directory appropriately.\n",
    "\n",
    "We will use `np.loadtxt` to read the data, skipping the first 4 rows (which contain metadata in all SNAP datasets). Note that for the sake of clarity we will forego some smarter, vectorized data manipulation operations you could do in `numpy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = './'\n",
    "FILE_NAME = 'email-Eu-core.txt'\n",
    "mat = np.loadtxt(DATA_PATH + FILE_NAME, dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have loaded the edgelist, we can inspect it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [2, 3],\n",
       "       [2, 4]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat[:3,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the nodes are zero-indexed. We will need to keep track of the total number of nodes and edges, so we should take that factor into account when counting nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1005\n",
      "25571\n"
     ]
    }
   ],
   "source": [
    "num_nodes = int(np.max(mat)) + 1\n",
    "num_edges = mat.shape[0]\n",
    "print(num_nodes)\n",
    "print(num_edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have an edgelist, but this only represents the 1s in our matrix. We also need to find all the zeros. We will do so by traversing our adjacency matrix exhaustively and finding all the pairs $(i, j)$ which are not 1s. Note that this is a very expensive operation for large graphs -- but do not worry! In practice, it is enough to *sample* from among the zeros, and what we're doing right now is a bit overkill. Since a graph with 1000-ish nodes has just over 1 million possible edges, we can afford to look at all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ones = set([tuple(mat[i,:].tolist()) for i in range(num_edges)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zeros = [(i, j)\n",
    "         for i in range(num_nodes)\n",
    "         for j in range(num_nodes)\n",
    "         if i != j and not (i, j) in ones]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ones = list(ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(86, 820), (185, 101), (155, 589), (473, 418), (87, 459), (66, 66), (513, 513), (218, 313), (329, 652), (271, 305)]\n",
      "[(0, 2), (0, 3), (0, 4), (0, 7), (0, 8), (0, 9), (0, 10), (0, 11), (0, 12), (0, 13)]\n"
     ]
    }
   ],
   "source": [
    "print(ones[:10])\n",
    "print(zeros[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25571\n",
      "984091\n"
     ]
    }
   ],
   "source": [
    "print(len(ones))\n",
    "print(len(zeros))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need one more thing to generate our logistic regression dataset -- the weight $w_{i, $}$, set at 1 for 1s and at $N_1 / N_0$ for 0s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zeros_weight = len(ones) * 1.0 / len(zeros)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to create our dataset, a big matrix with 4 columns: two columns for the left- and right-hand side IDs, one column for the weight, and one column for the labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = [list(x) + [zeros_weight, 0] for x in zeros] + [list(x) + [1, 1] for x in ones]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 2, 0.025984385590356988, 0], [0, 3, 0.025984385590356988, 0], [0, 4, 0.025984385590356988, 0], [0, 7, 0.025984385590356988, 0], [0, 8, 0.025984385590356988, 0], [0, 9, 0.025984385590356988, 0], [0, 10, 0.025984385590356988, 0], [0, 11, 0.025984385590356988, 0], [0, 12, 0.025984385590356988, 0], [0, 13, 0.025984385590356988, 0]]\n"
     ]
    }
   ],
   "source": [
    "print(data[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing PyTorch datatypes\n",
    "\n",
    "The essential data structure in PyTorch is called a \"tensor.\" It's similar to an `ndarray` in `numpy`: the n-dimensional homologue of a matrix. The difference is that tensor operations are *really* fast in PyTorch, which is optimized to do these things very quickly on both the CPU and GPU.\n",
    "\n",
    "Speaking of which, moving tensors between the computer's main RAM and the GPU RAM is extremely easy in PyTorch, a feature which is nothing short of magical. We won't use this feature just yet, however. When first developing a model it makes sense to keep things on the CPU, where the errors are generally a lot more informative.\n",
    "\n",
    "It's really easy to create this tensor -- just call the `FloatTensor` constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tensor = torch.FloatTensor(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0.0000,     2.0000,     0.0260,     0.0000],\n",
      "        [    0.0000,     3.0000,     0.0260,     0.0000],\n",
      "        [    0.0000,     4.0000,     0.0260,     0.0000],\n",
      "        ...,\n",
      "        [  116.0000,   548.0000,     1.0000,     1.0000],\n",
      "        [  815.0000,   880.0000,     1.0000,     1.0000],\n",
      "        [   63.0000,   255.0000,     1.0000,     1.0000]])\n"
     ]
    }
   ],
   "source": [
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets and Dataloaders\n",
    "\n",
    "PyTorch is built to deal with very large datasets, many of which will not really fit into memory. To elegantly isolate away the custom code required to load certain kinds of data, PyTorch introduces two classes -- a `Dataset` and a `DataLoader`.\n",
    "\n",
    "The `Dataset` class keeps track of where the data actually lives: in a file, a folder on disk, somewhere on the Internet, etc. It also defines the notion of an *example*: a row in a survey matrix, an image on disk, a time series, an audio recording, or a text document.\n",
    "\n",
    "In our simple case, our data lives in a tensor stored in memory, and a row in this tensor constitutes an example. This means we can use the `TensorDataset` class, which is effectively a wrapper around our `FloatTensor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = TensorDataset(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something to note: `dataset` is effectively a list of rows from `tensor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  0.0000,   2.0000,   0.0260,   0.0000],\n",
      "        [  0.0000,   3.0000,   0.0260,   0.0000],\n",
      "        [  0.0000,   4.0000,   0.0260,   0.0000],\n",
      "        [  0.0000,   7.0000,   0.0260,   0.0000],\n",
      "        [  0.0000,   8.0000,   0.0260,   0.0000],\n",
      "        [  0.0000,   9.0000,   0.0260,   0.0000],\n",
      "        [  0.0000,  10.0000,   0.0260,   0.0000],\n",
      "        [  0.0000,  11.0000,   0.0260,   0.0000],\n",
      "        [  0.0000,  12.0000,   0.0260,   0.0000],\n",
      "        [  0.0000,  13.0000,   0.0260,   0.0000]])\n"
     ]
    }
   ],
   "source": [
    "some_rows, = dataset[:10]\n",
    "print(some_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rows above appear in the same order as they were originally generated. This is a problem when using **minibatch Stochastic Gradient Descent**, the standard optimization algorithm that powers neural network research.\n",
    "\n",
    "Gradient descent as an optimization strategy works by taking small steps (slightly changing the values of the estimated parameters) in the direction which looks most promising, i.e. would most reduces the total loss. The gradient of the loss with respect to the model's parameters tells the model in which direction to head.\n",
    "\n",
    "The *exact* gradient would require the calculation of the [Jacobian matrix](https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant), the matrix of first-order partial derivatives with of the entire loss with respect to all parameters in our model. Given that we \"only\" have $2 * 1005 = 2010$ parameters and $1005^2$ examples in our toy model, it is technically possible to compute this matrix. But this matrix becomes very expensive to compute for \"real-world\" sized models which may have many millions of parameters.\n",
    "\n",
    "Instead, [Stochastic Gradient Descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) uses an approximation of the gradient, which is computed on a *mini-batch*, a subset of examples, which can comfortably fit into memory. And this mini-batch cannot have all 1s, or all 0s -- in that case it would be trivial to minimize the loss by making the model always predict all 1s or 0s. Instead, it is desirable to have a mini-batch that is a random sample from the dataset. Moreover, we will usually need to go through the same dataset multiple times (\"epochs\") as we iterate towards a solution, and in that case it is also desirable to re-randomize our data between epochs.\n",
    "\n",
    "The `DataLoader` class answers these requirements, by giving us a quick way to produce batches up to a maximum size from our `Dataset`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loader = DataLoader(dataset, batch_size=100000, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what does a `DataLoader` *actually* do? Everytime you call its `__iter__` method, a `DataLoader` gives back an iterator -- but this iterator is over *batches*, not over examples. The examples in the batches cover all the data in `Dataset`, but the order in which rows are distributed into batches is randomized (via `shuffle=True`).\n",
    "\n",
    "We can see the first batch yielded by the loader by calling `__next__` on the iterator it just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch = loader.__iter__().__next__()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  436.0000,   594.0000,     0.0260,     0.0000],\n",
      "        [  550.0000,   250.0000,     0.0260,     0.0000],\n",
      "        [  621.0000,   102.0000,     0.0260,     0.0000],\n",
      "        ...,\n",
      "        [  105.0000,   110.0000,     0.0260,     0.0000],\n",
      "        [  798.0000,   193.0000,     0.0260,     0.0000],\n",
      "        [  252.0000,   847.0000,     0.0260,     0.0000]])\n"
     ]
    }
   ],
   "source": [
    "print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Embedding module\n",
    "\n",
    "You can think of the features we will learn as numbers in a look-up table. When nodes $i$ and $j$ are under consideration, we look for the $i$-th and $j$-th entry in this table, get the vectors from there, take their cross product, run it through a sigmoid and we have a prediction for whether a tie is likely to exist or not between the two nodes. We also know whether a tie actually exists. If we did poorly at this prediction, the values in our lookup table may be in need of a lot of revision. If we did pretty well, maybe we'll move the values around just a little bit.\n",
    "\n",
    "The `Embedding` module in PyTorch expresses exactly this idea of a lookup table. We set it up by calling `torch.nn.Embedding` and indicating the number of entities in our graph, as well as the size of our feature vectors in the constructor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb = torch.nn.Embedding(num_nodes, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calls to the `Embedding` module require `LongTensor` node IDs, which must also be numbered from 0 to $n-1$, where $n$ is the number of vertices. In our case we obtain `LongTensor`s by first converting the first two columns in our batch to their `numpy` equivalent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "node_ids = torch.LongTensor(batch[:,:2].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  436,   594],\n",
      "        [  550,   250],\n",
      "        [  621,   102],\n",
      "        ...,\n",
      "        [  105,   110],\n",
      "        [  798,   193],\n",
      "        [  252,   847]])\n"
     ]
    }
   ],
   "source": [
    "print(node_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our `LongTensor` node IDs, we can issue lookups in the `Embedding` table for their vectors. We do this for the left- and right-hand side vectors separately. We retrieve vectors for the entire batch at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lhs = emb(node_ids[:, 0])\n",
    "rhs = emb(node_ids[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.0952, -0.3440],\n",
      "        [ 0.3098,  0.2339],\n",
      "        [-0.6589,  0.1565],\n",
      "        [-0.6285,  2.7189],\n",
      "        [ 0.0848,  0.4112],\n",
      "        [-0.2191,  0.1433],\n",
      "        [ 1.6574,  0.8133],\n",
      "        [-2.0182, -1.0061],\n",
      "        [ 0.1538, -0.1730],\n",
      "        [ 1.6420, -1.2262]])\n"
     ]
    }
   ],
   "source": [
    "print(lhs[:10,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's trivial to get the cross-product of the two vectors for every example in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.7618e+00,  6.3492e-01, -4.2345e-01,  ..., -1.0914e+00,\n",
      "         8.2291e-01, -1.6657e-01])\n"
     ]
    }
   ],
   "source": [
    "cross_prod = (lhs * rhs).sum(dim=1)\n",
    "print(cross_prod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Loss Function\n",
    "\n",
    "We will quantify how well (or poorly) we are doing at predicting the existence of edges in our graph by using the Binary Cross-Entropy loss. To do so, we will need a prediction, a label, and -- in our case -- a weight.\n",
    "\n",
    "First we turn our cross product into a prediction, by running it through a sigmoid transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0594,  0.6536,  0.3957,  ...,  0.2513,  0.6949,  0.4585])\n"
     ]
    }
   ],
   "source": [
    "pred = cross_prod.sigmoid()\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we also get labels as part of our batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = batch[:, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n"
     ]
    }
   ],
   "source": [
    "print(labels[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have computed predictions and retrieved our labels, we can then use the binary cross-entropy loss discussed in the introduction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "casewise_loss = -1 * ((labels * pred.log()) + (1-labels) * (1-pred).log())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 6.1263e-02,  1.0602e+00,  5.0367e-01,  ...,  2.8948e-01,\n",
      "         1.1870e+00,  6.1333e-01])\n"
     ]
    }
   ],
   "source": [
    "print(casewise_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the losses must be weighted according to whether the example had a 1 (rare) or a 0 (frequent) label. The batch loss is the weighted sum of casewise losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = batch[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4357.6221)\n"
     ]
    }
   ],
   "source": [
    "loss = (casewise_loss * weights).sum()\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Variable Class\n",
    "\n",
    "To really understand the power of PyTorch you need to understand the function of the `Variable` class. A `Variable` is a tensor that \"remembers\" what operations it participated in.\n",
    "\n",
    "What do I mean by this, and why is it useful? Let's go back to our loss function:\n",
    "\n",
    "$$ L = - \\sum_{i} \\sum_{j} w_{i, j} \\big[y_{i, j} \\log{p_{i, j}} + (1-y_{i, j})(\\log{1-p_{i,j}})\\big] $$\n",
    "\n",
    "The mini-batch SGD algorithm requires us to make a small step in the direction indicated by the gradient. The step has a \"width\" $\\alpha$, also known as the **learning rate**. The gradient tells us how much the loss *increases* if we increase a parameter by $\\alpha$. Thus, to minimize the loss, we will move in the opposite direction of the gradient. We will update all vectors $\\vec{v_i}$ using this equation:\n",
    "\n",
    "$$ \\vec{v}_i := \\vec{v}_i - \\alpha \\frac{\\partial L}{\\partial \\vec{v_i}} $$\n",
    "\n",
    "But how do we get this gradient, when all we have is a loss? This is where it's useful to rewrite our loss as follows:\n",
    "\n",
    "$$ L = - \\sum_{i} \\sum_{j} w_{i, j} \\times \\textrm{BCE}(y_{i, j}, \\textrm{sig}(\\vec{v_{i}} \\cdot \\vec{v_{j}})) $$\n",
    "\n",
    "To take the derivative $ \\partial L / \\partial \\vec{v_i} $, we need to go back to elementary calculus and remember that the derivative of a sum is the sum of the terms' derivatives, and that $w_{i, j}$ is a constant that does not depend on $\\vec{v_i}$:\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial \\vec{v_i}} = - \\sum_{i} \\sum_{j} w_{i, j} \\partial \\frac{\\textrm{BCE}(y_{i, j}, \\textrm{sig}(\\vec{v_{i}} \\cdot \\vec{v_{j}}))}{\\partial \\vec{v_i}} $$\n",
    "\n",
    "Okay, this was the easy part. Now we need to get the gradient of the binary cross-entropy function with respect to our vector. To do so, we can enlist the [chain rule](https://en.wikipedia.org/wiki/Chain_rule), which stipulates that:\n",
    "\n",
    "$$ (f(g(x)))' = f'(g(x))g'(x) $$\n",
    "\n",
    "It would be a long and rather tedious exercise to go into exactly how the chain rule is applied in our particular case, but suffice to say that for certain derivatives $f'(x)$ you need to know what the value of $x$ is in order to compute the derivative. But note how we have already computed our loss: for every function along the chain leading to our final loss value, we had to compute this $x$. The `Variable` class \"remembers\" this value where applicable, and uses it to give back a gradient.\n",
    "\n",
    "Of course, `Variables` can only be subject to differentiable operations, the logic for which has to be implemented and optimized. But there are a lot of differentiable operations available in PyTorch -- and because of the chain rule, `Variable`s can be composed into the endlessly complex chains of operations that power modern AI models.\n",
    "\n",
    "So how do we use Variables? Let's rewrite our loss calculation with them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "def compute_loss(batch):\n",
    "    lhs_node_ids = torch.LongTensor(batch[:,0].numpy())\n",
    "    rhs_node_ids = torch.LongTensor(batch[:,1].numpy())\n",
    "\n",
    "    lhs_var = Variable(lhs_node_ids)\n",
    "    rhs_var = Variable(rhs_node_ids)\n",
    "    labels = Variable(batch[:, 3])\n",
    "\n",
    "    lhs_vec = emb(lhs_var)\n",
    "    rhs_vec = emb(rhs_var)\n",
    "    cross_prod = (lhs_vec * rhs_vec).sum(dim=1)\n",
    "    pred = cross_prod.sigmoid()\n",
    "    casewise_loss = -1 * ((labels * pred.log()) + (1-labels) * (1-pred).log())\n",
    "    weights = batch[:, 2]\n",
    "    loss = (casewise_loss * weights).sum()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was... not so hard. In fact the use of `Variable` is so straightforward that PyTorch 0.4 has deprecated their use, and just unified them with `Tensor` types. But in my view it's still educational to look at the enhancement this data structure brings as separate from what `Tensor`s do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizers\n",
    "\n",
    "Remember how for SGD we'd do a parameter update following this formula:\n",
    "\n",
    "$$ \\vec{v}_i := \\vec{v}_i - \\alpha \\frac{\\partial L}{\\partial \\vec{v_i}} $$\n",
    "\n",
    "This is pretty straightforward in theory, but it's actually quite nasty in practice, since we'd have to visit every single parameter in our model. The model presented here is extremely simple, but nonetheless it would potentially be quite tedious to update every single parameter, depending on how our gradients are calculated. Moreover, there are many \"better\" [versions of SGD](http://ruder.io/optimizing-gradient-descent/), such as AdaGrad or Adam that get models to converge faster. These algorithms often require complex parameter updates that are best abstracted away -- which is exactly what PyTorch does. So how does an optimizer operate? We can find out by writing a few lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optim = torch.optim.SGD(emb.parameters(), lr=10e-3)\n",
    "\n",
    "def learn_from_batch(batch):\n",
    "    optim.zero_grad()\n",
    "    loss = compute_loss(batch)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's happening in this function?\n",
    "\n",
    "First we initialize our optimizer, and give it jurisdiction over our Embedding table's parameters. We also set a learning rate -- 10e-3 seems like a good value to start with, though this parameter will need to be tuned. Note that different optimizers could have jurisdiction over different model parameters.\n",
    "\n",
    "Then, in the function, the optimizer object begins by zeroing its gradients (resetting its state). Then we compute the loss, and then the gradient computation happens through `loss.backward()`. This is where the magic occurs: we do not need to write a single line of code indicating how gradients are to be computed -- the PyTorch internals take care of all of this. Then, finally, a call to `step` ensures the parameter update. (A clear explanation of how this works in a 2-layer neural network can be found [here](https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_optim.html)).\n",
    "\n",
    "To see how the parameters change, we can inspect them before and after the training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 5.8352e-01,  2.3258e+00],\n",
      "        [ 5.3681e-01,  1.1407e+00],\n",
      "        [-1.0678e+00, -7.1846e-01],\n",
      "        ...,\n",
      "        [ 2.9300e-01,  4.2089e-01],\n",
      "        [-1.5026e-01, -3.9012e-01],\n",
      "        [-1.1051e+00, -5.9288e-01]])\n",
      "Parameter containing:\n",
      "tensor([[ 0.5498,  2.3030],\n",
      "        [ 0.5199,  1.1350],\n",
      "        [-1.0439, -0.6840],\n",
      "        ...,\n",
      "        [ 0.2899,  0.4196],\n",
      "        [-0.1552, -0.3634],\n",
      "        [-1.0928, -0.5847]])\n"
     ]
    }
   ],
   "source": [
    "print(emb.weight)\n",
    "learn_from_batch(batch)\n",
    "print(emb.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look closely, you will see that parameters only changed a little, or not a at all, but the values did shift. How fast the parameters need to change depends on the specifics of our model -- but seing shifts in their values is a sign the `optim` object is doing its job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training for an epoch\n",
    "\n",
    "Now that we have figured out how to deal with a single batch, it's time to run our algorithm for an entire epoch, going through every single batch in our data and iteratively updating our parameters. Our `DataLoader` makes this quite easy. In our implementation we will keep track of the average loss per case, which should go down as our model learns. Given that every batch only contains a fraction of the data, we do expect the per-batch loss to jump around a little. We return a per-epoch loss, which will be very informative as we train our model for multiple epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(verbose=True):\n",
    "    total_loss = 0\n",
    "    for batch, in loader:\n",
    "        loss = learn_from_batch(batch)\n",
    "        if verbose:\n",
    "            per_case_loss = loss / batch.size()[0]\n",
    "            print(per_case_loss)\n",
    "        total_loss += loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.00000e-02 *\n",
      "       4.2976)\n",
      "tensor(1.00000e-02 *\n",
      "       4.2168)\n",
      "tensor(1.00000e-02 *\n",
      "       4.2238)\n",
      "tensor(1.00000e-02 *\n",
      "       4.1562)\n",
      "tensor(1.00000e-02 *\n",
      "       4.1015)\n",
      "tensor(1.00000e-02 *\n",
      "       4.1067)\n",
      "tensor(1.00000e-02 *\n",
      "       4.0838)\n",
      "tensor(1.00000e-02 *\n",
      "       4.0098)\n",
      "tensor(1.00000e-02 *\n",
      "       4.0070)\n",
      "tensor(1.00000e-02 *\n",
      "       3.9915)\n",
      "tensor(1.00000e-02 *\n",
      "       3.9275)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(41574.2266)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_one_epoch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moving to the GPU\n",
    "\n",
    "If easy tensor multiplication and automated differentiation weren't great enough, PyTorch also offers a painless way to move data and computation between the CPU and the GPU. PyTorch supports NVIDIA GPUs that run CUDA, a C++-based programming language for GPU-based computation. An incidental outcome of the requirements of computer graphics, GPUs turn out to be really fast at tensor multiplication. GPUs (and CUDA) are also, typically a pain to work with, which is why the availability of a means to work with them while abstracting away their details is a great help. To see what we need to move to CUDA, let's get together our entire learning routine.\n",
    "\n",
    "Note: if you don't have a GPU available, just set `GPU_ENABLED` to False!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GPU_ENABLED = True\n",
    "\n",
    "emb = torch.nn.Embedding(num_nodes, 2)\n",
    "if GPU_ENABLED:\n",
    "    emb = emb.cuda()\n",
    "optim = torch.optim.SGD(emb.parameters(), lr=10e-3)\n",
    "\n",
    "def compute_loss(batch, emb=emb, optim=optim):\n",
    "    lhs_node_ids = torch.LongTensor(batch[:,0].numpy())\n",
    "    rhs_node_ids = torch.LongTensor(batch[:,1].numpy())\n",
    "\n",
    "    lhs_var = Variable(lhs_node_ids)\n",
    "    rhs_var = Variable(rhs_node_ids)\n",
    "    labels = Variable(batch[:, 3])\n",
    "    weights = batch[:, 2]\n",
    "    \n",
    "    if GPU_ENABLED:\n",
    "        lhs_var = lhs_var.cuda()\n",
    "        rhs_var = rhs_var.cuda()\n",
    "        labels = labels.cuda()\n",
    "        weights = weights.cuda()\n",
    "        \n",
    "    lhs_vec = emb(lhs_var)\n",
    "    rhs_vec = emb(rhs_var)\n",
    "    cross_prod = (lhs_vec * rhs_vec).sum(dim=1)\n",
    "    pred = cross_prod.sigmoid()\n",
    "    casewise_loss = -1 * ((labels * pred.log()) + (1-labels) * (1-pred).log())\n",
    "    loss = (casewise_loss * weights).sum()\n",
    "    return loss\n",
    "\n",
    "def learn_from_batch(batch, emb=emb, optim=optim):\n",
    "    optim.zero_grad()\n",
    "    loss = compute_loss(batch, emb, optim)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    return loss\n",
    "\n",
    "def train_one_epoch(verbose=True, emb=emb, optim=optim):\n",
    "    total_loss = 0\n",
    "    for batch, in loader:\n",
    "        loss = learn_from_batch(batch, emb, optim)\n",
    "        if verbose:\n",
    "            per_case_loss = loss / batch.size()[0]\n",
    "            print(per_case_loss)\n",
    "        total_loss += loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There were 5 things we needed to move to the GPU. In all cases, this was achieved by calling `.cuda()` on a data structure. Specifically, we had to move:\n",
    "- the `Embedding` module. This is our \"model\", which contains the parameters we will learn.\n",
    "- the `lhs_var` and `rhs_var` indices. These contain the indices we actually look up in each batch. Note that a lookup is a differentiable operation!\n",
    "- the `labels` and `weights`, which are both crucial parts of our loss.\n",
    "\n",
    "That's it! We did not really need to explicitly do anything to intermediary data, or to the `optim` object -- these objects can infer what memory they need to use from context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.00000e-02 *\n",
      "       4.3301, device='cuda:0')\n",
      "tensor(1.00000e-02 *\n",
      "       4.2964, device='cuda:0')\n",
      "tensor(1.00000e-02 *\n",
      "       4.2556, device='cuda:0')\n",
      "tensor(1.00000e-02 *\n",
      "       4.1789, device='cuda:0')\n",
      "tensor(1.00000e-02 *\n",
      "       4.1121, device='cuda:0')\n",
      "tensor(1.00000e-02 *\n",
      "       4.0933, device='cuda:0')\n",
      "tensor(1.00000e-02 *\n",
      "       4.0227, device='cuda:0')\n",
      "tensor(1.00000e-02 *\n",
      "       4.1015, device='cuda:0')\n",
      "tensor(1.00000e-02 *\n",
      "       3.9905, device='cuda:0')\n",
      "tensor(1.00000e-02 *\n",
      "       3.9384, device='cuda:0')\n",
      "tensor(1.00000e-02 *\n",
      "       4.1248, device='cuda:0')\n",
      "tensor(41717.8984, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(train_one_epoch())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bringing it all together\n",
    "\n",
    "Running on CUDA means we can typically spend about 90% less time waiting for our computation to finish -- a pretty significant difference. Now it's time to run our algorithm for multiple epochs, only to encounter a small surprise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss=42364.4102\n",
      "Epoch: 1, loss=38916.1836\n",
      "Epoch: 2, loss=37250.2344\n",
      "Epoch: 3, loss=36074.9805\n",
      "Epoch: 4, loss=34930.3203\n",
      "Epoch: 5, loss=33692.7500\n",
      "Epoch: 6, loss=32461.7031\n",
      "Epoch: 7, loss=31370.2910\n",
      "Epoch: 8, loss=30470.7090\n",
      "Epoch: 9, loss=29747.5156\n",
      "Epoch: 10, loss=29162.0645\n",
      "Epoch: 11, loss=28685.4688\n",
      "Epoch: 12, loss=28291.5469\n",
      "Epoch: 13, loss=27963.9688\n",
      "Epoch: 14, loss=27689.9902\n",
      "Epoch: 15, loss=27459.1152\n",
      "Epoch: 16, loss=27264.0410\n",
      "Epoch: 17, loss=27097.5371\n",
      "Epoch: 18, loss=26955.2676\n",
      "Epoch: 19, loss=26832.0664\n",
      "Epoch: 20, loss=26725.1895\n",
      "Epoch: 21, loss=26630.8027\n",
      "Epoch: 22, loss=26547.7988\n",
      "Epoch: 23, loss=26474.3750\n",
      "Epoch: 24, loss=26409.8242\n",
      "Epoch: 25, loss=26350.9531\n",
      "Epoch: 26, loss=26298.5156\n",
      "Epoch: 27, loss=26251.5977\n",
      "Epoch: 28, loss=26209.1270\n",
      "Epoch: 29, loss=26170.9961\n",
      "Epoch: 30, loss=26135.6484\n",
      "Epoch: 31, loss=26104.6270\n",
      "Epoch: 32, loss=26075.8516\n",
      "Epoch: 33, loss=26049.2207\n",
      "Epoch: 34, loss=26024.9805\n",
      "Epoch: 35, loss=26002.6035\n",
      "Epoch: 36, loss=25981.5176\n",
      "Epoch: 37, loss=25962.1758\n",
      "Epoch: 38, loss=25943.9414\n",
      "Epoch: 39, loss=25926.8184\n",
      "Epoch: 40, loss=25910.5566\n",
      "Epoch: 41, loss=25895.0430\n",
      "Epoch: 42, loss=25881.6387\n",
      "Epoch: 43, loss=25867.4922\n",
      "Epoch: 44, loss=25854.3047\n",
      "Epoch: 45, loss=25842.2031\n",
      "Epoch: 46, loss=25829.9863\n",
      "Epoch: 47, loss=25818.9863\n",
      "Epoch: 48, loss=25808.1016\n",
      "Epoch: 49, loss=25798.6445\n",
      "Epoch: 50, loss=nan\n",
      "Epoch: 51, loss=nan\n",
      "Epoch: 52, loss=nan\n",
      "Epoch: 53, loss=nan\n",
      "Epoch: 54, loss=nan\n",
      "Epoch: 55, loss=nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-7d5b4382051d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch: %d, loss=%.4f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-40-79f5c5b045e2>\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(verbose, emb, optim)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearn_from_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "emb = torch.nn.Embedding(num_nodes, 2).cuda()\n",
    "optim = torch.optim.SGD(emb.parameters(), lr=10e-3)\n",
    "\n",
    "for i in range(100):\n",
    "    loss = train_one_epoch(verbose=False, emb=emb, optim=optim)\n",
    "    print(\"Epoch: %d, loss=%.4f\" % (i, loss.cpu().detach().numpy().tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why the loss went to nan?\n",
    "\n",
    "It's worth going back to the definition of the cross-entropy function. There's a `log(p)` and a `log(1-p)` in there. This means that if we do *really* well at predicting that two nodes will or won't have an edge, we could have some huge effects on the loss, at least as far as the gradient is concerned. This is a problem known as the \"exploding gradient problem\" in deep learning, but it can apply to feature learning as well. \n",
    "\n",
    "The solution is to prevent the model from being tempted to do \"really\" well at the prediction task. We can do so by changing the loss function to \"clamp\" predictions at a minimum and a maximum value, $\\epsilon = 10^{-6}$. The model will only optimize the predictions in the interval $[\\epsilon, 1-\\epsilon]$, and ignore any values outside this interval.\n",
    "\n",
    "Note that regularization would be another effective remedy in this case, but it's not strictly necessary to solve this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_loss(batch, emb=emb, optim=optim):\n",
    "    lhs_node_ids = torch.LongTensor(batch[:,0].numpy())\n",
    "    rhs_node_ids = torch.LongTensor(batch[:,1].numpy())\n",
    "\n",
    "    lhs_var = Variable(lhs_node_ids).cuda()\n",
    "    rhs_var = Variable(rhs_node_ids).cuda()\n",
    "    labels = Variable(batch[:, 3]).cuda()\n",
    "\n",
    "    lhs_vec = emb(lhs_var)\n",
    "    rhs_vec = emb(rhs_var)\n",
    "    cross_prod = (lhs_vec * rhs_vec).sum(dim=1)\n",
    "    \n",
    "    EPS = 1e-6\n",
    "    pred = cross_prod.sigmoid().clamp(min=EPS, max=1-EPS)\n",
    "    casewise_loss = -1 * ((labels * pred.log()) + (1-labels) * (1-pred).log())\n",
    "    weights = batch[:, 2].cuda()\n",
    "    \n",
    "    loss = (casewise_loss * weights).sum()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to run our training one more time, this time using clamped predictions. We will also keep track of the predicted weights at each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss=42646.9492\n",
      "Epoch: 1, loss=39179.1797\n",
      "Epoch: 2, loss=37629.4180\n",
      "Epoch: 3, loss=36702.6094\n",
      "Epoch: 4, loss=35954.9375\n",
      "Epoch: 5, loss=35158.6328\n",
      "Epoch: 6, loss=34184.7578\n",
      "Epoch: 7, loss=33054.9023\n",
      "Epoch: 8, loss=31919.5879\n",
      "Epoch: 9, loss=30920.0293\n",
      "Epoch: 10, loss=30102.5078\n",
      "Epoch: 11, loss=29451.1094\n",
      "Epoch: 12, loss=28930.7188\n",
      "Epoch: 13, loss=28508.1113\n",
      "Epoch: 14, loss=28158.3984\n",
      "Epoch: 15, loss=27863.4863\n",
      "Epoch: 16, loss=27611.9727\n",
      "Epoch: 17, loss=27392.4590\n",
      "Epoch: 18, loss=27197.0684\n",
      "Epoch: 19, loss=27022.6680\n",
      "Epoch: 20, loss=26866.4473\n",
      "Epoch: 21, loss=26725.8086\n",
      "Epoch: 22, loss=26598.5684\n",
      "Epoch: 23, loss=26484.9180\n",
      "Epoch: 24, loss=26383.4668\n",
      "Epoch: 25, loss=26294.8262\n",
      "Epoch: 26, loss=26217.2012\n",
      "Epoch: 27, loss=26149.8750\n",
      "Epoch: 28, loss=26091.4062\n",
      "Epoch: 29, loss=26040.8691\n",
      "Epoch: 30, loss=25997.8457\n",
      "Epoch: 31, loss=25960.6758\n",
      "Epoch: 32, loss=25929.1680\n",
      "Epoch: 33, loss=25901.5469\n",
      "Epoch: 34, loss=25877.8125\n",
      "Epoch: 35, loss=25857.9082\n",
      "Epoch: 36, loss=25840.0664\n",
      "Epoch: 37, loss=25823.9941\n",
      "Epoch: 38, loss=25811.3535\n",
      "Epoch: 39, loss=25799.1562\n",
      "Epoch: 40, loss=25787.5332\n",
      "Epoch: 41, loss=25778.9043\n",
      "Epoch: 42, loss=25769.4648\n",
      "Epoch: 43, loss=25761.9824\n",
      "Epoch: 44, loss=25754.9102\n",
      "Epoch: 45, loss=25748.3359\n",
      "Epoch: 46, loss=25742.5195\n",
      "Epoch: 47, loss=25737.0391\n",
      "Epoch: 48, loss=25731.4883\n",
      "Epoch: 49, loss=25726.9355\n",
      "Epoch: 50, loss=25722.1680\n",
      "Epoch: 51, loss=25718.7148\n",
      "Epoch: 52, loss=25713.8730\n",
      "Epoch: 53, loss=25709.6250\n",
      "Epoch: 54, loss=25706.2617\n",
      "Epoch: 55, loss=25702.6504\n",
      "Epoch: 56, loss=25699.3770\n",
      "Epoch: 57, loss=25694.9316\n",
      "Epoch: 58, loss=25691.7285\n",
      "Epoch: 59, loss=25688.5586\n",
      "Epoch: 60, loss=25684.7148\n",
      "Epoch: 61, loss=25681.5469\n",
      "Epoch: 62, loss=25678.3418\n",
      "Epoch: 63, loss=25674.4629\n",
      "Epoch: 64, loss=25671.1602\n",
      "Epoch: 65, loss=25668.4590\n",
      "Epoch: 66, loss=25665.5430\n",
      "Epoch: 67, loss=25662.4746\n",
      "Epoch: 68, loss=25659.4082\n",
      "Epoch: 69, loss=25656.4160\n",
      "Epoch: 70, loss=25652.8379\n",
      "Epoch: 71, loss=25651.2773\n",
      "Epoch: 72, loss=25648.4160\n",
      "Epoch: 73, loss=25644.9863\n",
      "Epoch: 74, loss=25643.7520\n",
      "Epoch: 75, loss=25640.7383\n",
      "Epoch: 76, loss=25638.7227\n",
      "Epoch: 77, loss=25635.8418\n",
      "Epoch: 78, loss=25634.8398\n",
      "Epoch: 79, loss=25632.9648\n",
      "Epoch: 80, loss=25630.5840\n",
      "Epoch: 81, loss=25629.6836\n",
      "Epoch: 82, loss=25627.6758\n",
      "Epoch: 83, loss=25626.8164\n",
      "Epoch: 84, loss=25625.5000\n",
      "Epoch: 85, loss=25624.3125\n",
      "Epoch: 86, loss=25623.3828\n",
      "Epoch: 87, loss=25622.1582\n",
      "Epoch: 88, loss=25621.0801\n",
      "Epoch: 89, loss=25620.7734\n",
      "Epoch: 90, loss=25619.7266\n",
      "Epoch: 91, loss=25618.8535\n",
      "Epoch: 92, loss=25618.6016\n",
      "Epoch: 93, loss=25617.6484\n"
     ]
    }
   ],
   "source": [
    "emb_weights = []\n",
    "emb = torch.nn.Embedding(num_nodes, 2).cuda()\n",
    "optim = torch.optim.SGD(emb.parameters(), lr=10e-3)\n",
    "\n",
    "for i in range(100):\n",
    "    loss = train_one_epoch(verbose=False, emb=emb, optim=optim)\n",
    "    emb_weights += [emb.weight.detach().cpu().numpy().tolist()]\n",
    "    print(\"Epoch: %d, loss=%.4f\" % (i, loss.cpu().detach().numpy().tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the predictions\n",
    "\n",
    "The choice of embeddings of size-2 was a bit sneaky. In practice we would not really consider such low-dimension embeddings to be viable, but in this case they are perfect for visualization. Visualization is done best in R using the `ggnetwork` and `gganimate` packages, and we will pursue this task in a [separate R notebook](visualize_emb.ipynb), but not after we write our data to a .tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('embedded_graph.tsv', 'w') as f:\n",
    "    for i in range(100):\n",
    "        wt = emb_weights[i]\n",
    "        for j in range(len(wt)):\n",
    "            f.write(\"%d\\t%d\\t%.6f\\t%.6f\\n\" % (i, j, wt[j][0], wt[j][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what the result looks like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"embedding_animation.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further extensions\n",
    "\n",
    "We won't be discussing these in this notebook, but there are quite a few things we still need to do before we can solve our feature learning problem for large graphs. \n",
    "\n",
    "- we need more dimensions in our embedding space! 2 is not really enough.\n",
    "- sampling negatives: notice how we have considered all the 0s when setting up our dataset. Ordinarily we would only consider about as many 0s as 1s. Note that sampling at random will not be a good idea, since in that case we'd just learn which nodes have high degree. There's an entire science to figuring out how to pick the right sample of negatives -- things could get really complicated if we were embedding weighted graphs, signed graphs or hypergraphs!\n",
    "- regularization: we would likely also want to apply some sort of regularization to our parameters if we will increase the dimensionality of our space, to prevent overfitting.\n",
    "- \"shingling\": for very large graphs, sampling negatives is not enough to make the problem tractable. We are bound by however many 32-bit floats can fit in our GPU's memory. There are however techniques for breaking this problem down into smaller sub-problems, which can all fit into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
